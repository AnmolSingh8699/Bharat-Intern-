{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded1e7d1",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fbe0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50c443",
   "metadata": {},
   "source": [
    "Here, we import the required libraries:\n",
    "\n",
    "os: Used for operating system dependent functionality (e.g., file paths).\n",
    "\n",
    "numpy: Numerical computing library for handling arrays and matrices.\n",
    "\n",
    "tensorflow and keras: Deep learning libraries for building and training neural networks.\n",
    "\n",
    "ImageDataGenerator: Helps generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "Sequential, Conv2D, MaxPooling2D, Flatten, Dense: Components for building the CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d940a3b",
   "metadata": {},
   "source": [
    "# Define data directories and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540add9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'# paths to the directories containing the training and validation images\n",
    "validation_dir = 'train'\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "input_shape = (img_width, img_height, 3)\n",
    "epochs = 10\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6ba84",
   "metadata": {},
   "source": [
    "Define the directories where the training and validation data are stored. Set the image dimensions, input shape, number of epochs for training, and batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac1761",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402e8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 2 classes.\n",
      "Found 557 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# rescale=1./255 is used to rescale the pixel values of the images to the range [0, 1].\n",
    "#This normalization is performed by dividing each pixel value by 255.\n",
    "#This step is important for neural networks as it helps in stabilizing the training process and improves convergence.\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),# dimensions to which all images will be resized during \n",
    "    batch_size=batch_size, # the number of images in each batch of data.\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')# tells type of label arrays that are returned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d5056",
   "metadata": {},
   "source": [
    "Data generators are created for both training and validation sets. These generators will load images from the specified directories, resize them to the target size, and normalize their pixel values. The flow_from_directory method generates batches of augmented data. In this case, since it's a binary classification problem (cats and dogs), class_mode is set to 'binary'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50816ae5",
   "metadata": {},
   "source": [
    "#  Build CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f98f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([ \n",
    "# initialize a sequential model, which is a linear stack of layers,layers are added one by one, and output of each layer feed into the next.\n",
    "   \n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    # adds 2D convolutional layer to the model(filters). \n",
    "    # 32: Number of filters (output channels).\n",
    "    # (3, 3): Kernel size, specifying the height and width of the 2D convolution window.\n",
    "    # activation='relu': Activation function applied element-wise to the output.\n",
    "    # input_shape= Specifies the shape of the input data. This is the shape of the input images (height, width, channels).\n",
    "    \n",
    "    MaxPooling2D((2, 2)), \n",
    "    # specifies size of the pooling window or filter \n",
    "    # adds a max-pooling layer to the model, downsample feature maps while retaining the most important information.\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    # filters are increased from 32 to 64\n",
    "    \n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    #filters are increased from 64 to 128\n",
    "    \n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    # flattens input, means it converts 2D feature maps into a 1D vector, prepares data for input into the subsequent fully connected layers.\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "# Dense: This is a fully connected layer in a neural network. \n",
    "# In dense layer, each neuron is connected to every neuron in the previous layer.\n",
    "# 512: specifies no. of neurons in layer. Each neuron in the dense layer receives input from all neurons in previous layer and it produces a single output.\n",
    "# activation='relu': activation function applied to  output of each neuron in the dense layer.\n",
    "# the Rectified Linear Unit (ReLU) activation function is used, introduces non-linearity into the model, allowing it to learn complex patterns and relationships in the data. Mathematically, ReLU activation function is defined as ReLU activation function replaces all negative values with zero, while leaving positive values unchanged.\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# output layer with a single neuron and sigmoid activation function. For binary classification problems, sigmoid activation is commonly used to produce a probability output in the range [0, 1],\n",
    "# where values closer to 1 represent one class (e.g., 'dog') and values closer to 0 represent the other class (e.g., 'cat').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ea4f3",
   "metadata": {},
   "source": [
    "A sequential model is created, representing a linear stack of layers. The model consists of several convolutional layers with max-pooling layers to extract features, followed by a flattening layer to convert the 2D feature maps into a 1D vector, and dense layers for classification. ReLU activation is used for all convolutional layers, and a sigmoid activation is used for the output layer for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec096a",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894d117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "# optimization algorithm to be used during the training of the neural network, refers to the Adam optimization algorithm\n",
    "              loss='binary_crossentropy',\n",
    "# measures difference b/w true labels and  predicted probabilities for binary classification tasks. Binary cross-entropy is for problems where output of the model is a probability distribution over two classes.              \n",
    "              metrics=['accuracy'])\n",
    "# measures the proportion of correctly classified samples among the total number of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c2c42",
   "metadata": {},
   "source": [
    "The model is compiled with the Adam optimizer, binary cross-entropy loss function (suitable for binary classification), and accuracy metric for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd92ff",
   "metadata": {},
   "source": [
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ceaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "17/17 [==============================] - 44s 2s/step - loss: 0.7242 - accuracy: 0.4762 - val_loss: 0.6947 - val_accuracy: 0.4908\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.6936 - accuracy: 0.5029 - val_loss: 0.6928 - val_accuracy: 0.5018\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.6928 - accuracy: 0.5048 - val_loss: 0.6905 - val_accuracy: 0.5607\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 41s 2s/step - loss: 0.6905 - accuracy: 0.5314 - val_loss: 0.6855 - val_accuracy: 0.5864\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.6993 - accuracy: 0.5486 - val_loss: 0.7453 - val_accuracy: 0.5018\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.6797 - accuracy: 0.5790 - val_loss: 0.6688 - val_accuracy: 0.6176\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.6656 - accuracy: 0.5657 - val_loss: 0.6515 - val_accuracy: 0.6121\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.6276 - accuracy: 0.6514 - val_loss: 0.6207 - val_accuracy: 0.6360\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.6102 - accuracy: 0.6819 - val_loss: 0.5617 - val_accuracy: 0.7132\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.5780 - accuracy: 0.7200 - val_loss: 0.5924 - val_accuracy: 0.6618\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "# fit() func. trains model on the provided training data and evaluates its performance on the validation data.\n",
    "    train_generator,\n",
    " # train_generator yields batches of training samples during each iteration of training.\n",
    "    \n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "#  specifies the no. of steps to be taken during each epoch of training. It is calculated as the total no.of training samples divided by the batch size. This ensures that the model processes the entire training dataset in each epoch.\n",
    "  \n",
    "    epochs=epochs,\n",
    "# epoch is one complete pass through entire training dataset. The model will iterate over the training data for the specified number of epochs, updating its weights based on the optimization algorithm and the loss function \n",
    "   \n",
    "    validation_data=validation_generator,\n",
    "# yields batches of validation samples for evaluating the model's performance after each epoch of training.\n",
    "    \n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "# tells the no.of steps to be taken during each epoch of validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a27b5d",
   "metadata": {},
   "source": [
    "The model is trained using the fit method. It takes the training generator, steps per epoch, number of epochs, validation generator, and validation steps as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42041e42",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6bb9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 15s 848ms/step - loss: 0.5927 - accuracy: 0.6607\n",
      "Validation Accuracy: 66.07%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "# variable stores the value of the loss function, loss value indicates how well model is performing on the validation data, with lower values indicating better performance.\n",
    "# After evaluation, variable stores the value of the accuracy metric computed on the validation dataset\n",
    "\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "# {:.2f}% is a placeholder for a floating-point number with two decimal places, % sign indicates that the accuracy will be displayed as a percentage , multiplies the accuracy value by 100 to convert it to a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51393ac8",
   "metadata": {},
   "source": [
    "The trained model is evaluated on the validation set, and the validation accuracy is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d4db1",
   "metadata": {},
   "source": [
    "In summary, the provided code can be used to train a CNN for the classification of images into two categories: dogs and cats. After training, the model can predict the probability of an input images belonging to each class, allowing it to classify new images accordingly if added.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
